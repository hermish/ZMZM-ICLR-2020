{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Multi-Image Dictionary\n",
    "\n",
    "## 5.1 Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from library import generator\n",
    "from cifar10_web import cifar10\n",
    "\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_to_image(pixels, patch_size, channels):\n",
    "    channel_patches = np.split(pixels, channels)\n",
    "    for channel in range(channels):\n",
    "        channel_patches[channel] = np.reshape(channel_patches[channel], (patch_size, patch_size))\n",
    "    patch = np.dstack(channel_patches)\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Figure Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_SIZE = 32\n",
    "CIFAR_CHANNELS = 3\n",
    "\n",
    "train_images, train_labels, _, _ = cifar10(path=None)\n",
    "total_images = len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 3\n",
    "PATCH_SIZE = 8\n",
    "TOTAL_PATCHES = 100000\n",
    "\n",
    "observations = np.zeros((PATCH_SIZE * PATCH_SIZE * CHANNELS, TOTAL_PATCHES))\n",
    "for col in range(TOTAL_PATCHES):\n",
    "    image_index = np.random.randint(total_images)\n",
    "    image = patch_to_image(train_images[image_index], CIFAR_SIZE, CIFAR_CHANNELS)\n",
    "    x_start = np.random.randint(CIFAR_SIZE - PATCH_SIZE)\n",
    "    y_start = np.random.randint(CIFAR_SIZE - PATCH_SIZE)\n",
    "    sample_patch = image[x_start:x_start + PATCH_SIZE, y_start:y_start + PATCH_SIZE,:]\n",
    "\n",
    "    slices = []\n",
    "    for channel in range(CHANNELS):\n",
    "        current = sample_patch[:,:,channel]\n",
    "        slices.append(np.ndarray.flatten(current))\n",
    "    observations[:,col] = np.concatenate(slices, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 200\n",
    "\n",
    "updates = generator.get_dictionary_learning_iterates(observations)\n",
    "dictionary = next(itertools.islice(updates, ITERATIONS, None))\n",
    "dictionary = dictionary.T\n",
    "encoding = dictionary.T @ observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIFAR BASES\n",
    "\n",
    "norms = np.abs(encoding)\n",
    "norms = np.sum(norms, axis=1)\n",
    "all_indices = list(range(len(norms)))\n",
    "all_indices.sort(key=lambda num: norms[num], reverse=True)\n",
    "\n",
    "sum_signs = np.sum(encoding, axis=1)\n",
    "sum_signs = np.sign(sum_signs)\n",
    "\n",
    "ROWS, COLS = 12, 16\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(64, 48))\n",
    "plt.subplots_adjust(left=None, right=None, bottom=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for index, ax in zip(all_indices[: ROWS * COLS], axs.flat):\n",
    "    base = dictionary[:,index] * sum_signs[index]\n",
    "    base = base - base.min()\n",
    "    base = base / base.max()\n",
    "    base = patch_to_image(base, PATCH_SIZE, CHANNELS)\n",
    "    \n",
    "    ax.imshow(base)\n",
    "    ax.axis('off')\n",
    "fig.savefig('05-cifar-bases.pdf', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOISY CIFAR BASES\n",
    "\n",
    "ITERATIONS = 200\n",
    "\n",
    "noisy_observations = observations + np.random.normal(scale=0.075, size=observations.shape)\n",
    "updates = generator.get_dictionary_learning_iterates(noisy_observations)\n",
    "noisy_dictionary = next(itertools.islice(updates, ITERATIONS, None))\n",
    "noisy_dictionary = noisy_dictionary.T\n",
    "noisy_encoding = noisy_dictionary.T @ noisy_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.abs(noisy_encoding)\n",
    "norms = np.sum(norms, axis=1)\n",
    "all_indices = list(range(len(norms)))\n",
    "all_indices.sort(key=lambda num: norms[num], reverse=True)\n",
    "\n",
    "sum_signs = np.sum(noisy_encoding, axis=1)\n",
    "sum_signs = np.sign(sum_signs)\n",
    "\n",
    "ROWS, COLS = 12, 16\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(64, 48))\n",
    "plt.subplots_adjust(left=None, right=None, bottom=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for index, ax in zip(all_indices, axs.flat):\n",
    "    base = noisy_dictionary[:,index] * sum_signs[index]\n",
    "    base = base - base.min()\n",
    "    base = base / base.max()\n",
    "    base = patch_to_image(base, PATCH_SIZE, CHANNELS)\n",
    "    \n",
    "    ax.imshow(base)\n",
    "    ax.axis('off')\n",
    "fig.savefig('05-noisy-cifar-bases.pdf', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRUPTED CIFAR BASES\n",
    "\n",
    "SIGMA = 0.5 * np.std(observations)\n",
    "BETA = 0.2\n",
    "ITERATIONS = 200\n",
    "\n",
    "plus_minus = (np.random.binomial(1, 0.5, size=observations.shape) - 1) * 2\n",
    "corruptions = np.random.binomial(1, BETA, size=observations.shape) * plus_minus * SIGMA\n",
    "corrupted_observations = observations + corruptions\n",
    "\n",
    "updates = generator.get_dictionary_learning_iterates(corrupted_observations)\n",
    "corrupted_dictionary = next(itertools.islice(updates, ITERATIONS, None))\n",
    "corrupted_dictionary = corrupted_dictionary.T\n",
    "corrupted_encoding = corrupted_dictionary.T @ corrupted_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.abs(corrupted_encoding)\n",
    "norms = np.sum(norms, axis=1)\n",
    "all_indices = list(range(len(norms)))\n",
    "all_indices.sort(key=lambda num: norms[num], reverse=True)\n",
    "\n",
    "sum_signs = np.sum(corrupted_encoding, axis=1)\n",
    "sum_signs = np.sign(sum_signs)\n",
    "\n",
    "ROWS, COLS = 12, 16\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(64, 48))\n",
    "plt.subplots_adjust(left=None, right=None, bottom=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for index, ax in zip(all_indices, axs.flat):\n",
    "    base = corrupted_dictionary[:,index] * sum_signs[index]\n",
    "    base = base - base.min()\n",
    "    base = base / base.max()\n",
    "    base = patch_to_image(base, PATCH_SIZE, CHANNELS)\n",
    "    \n",
    "    ax.imshow(base)\n",
    "    ax.axis('off')\n",
    "fig.savefig('05-corrupted-cifar-bases.pdf', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OUTLIERS CIFAR BASES\n",
    "\n",
    "TAU = 0.2\n",
    "MU = np.mean(observations)\n",
    "STDEV = 0.5 * np.std(observations)\n",
    "\n",
    "num_outliers = int(TAU * len(observations.T))\n",
    "noise_dimensions = (len(observations), num_outliers)\n",
    "outlier_observations = np.hstack((observations, np.random.normal(MU, STDEV, noise_dimensions)))\n",
    "\n",
    "updates = generator.get_dictionary_learning_iterates(outlier_observations)\n",
    "outlier_dictionary = next(itertools.islice(updates, ITERATIONS, None))\n",
    "outlier_dictionary = outlier_dictionary.T\n",
    "outlier_encoding = outlier_dictionary.T @ outlier_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.abs(outlier_encoding)\n",
    "norms = np.sum(norms, axis=1)\n",
    "all_indices = list(range(len(norms)))\n",
    "all_indices.sort(key=lambda num: norms[num], reverse=True)\n",
    "\n",
    "sum_signs = np.sum(outlier_encoding, axis=1)\n",
    "sum_signs = np.sign(sum_signs)\n",
    "\n",
    "ROWS, COLS = 12, 16\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(64, 48))\n",
    "plt.subplots_adjust(left=None, right=None, bottom=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for index, ax in zip(all_indices, axs.flat):\n",
    "    base = outlier_dictionary[:,index] * sum_signs[index]\n",
    "    base = base - base.min()\n",
    "    base = base / base.max()\n",
    "    base = patch_to_image(base, PATCH_SIZE, CHANNELS)\n",
    "    \n",
    "    ax.imshow(base)\n",
    "    ax.axis('off')\n",
    "fig.savefig('05-outlier-cifar-bases.pdf', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 6.2397774110002135\n"
     ]
    }
   ],
   "source": [
    "noise_stdev = 0.075\n",
    "snr_ratio = observations.mean() / noise_stdev\n",
    "print('SNR:', snr_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_BASES = 20\n",
    "\n",
    "norms = np.abs(encoding)\n",
    "norms = np.sum(norms, axis=1)\n",
    "clean_priorities = list(range(len(norms)))\n",
    "clean_priorities.sort(key=lambda row: norms[row], reverse=True)\n",
    "\n",
    "norms = np.abs(noisy_encoding)\n",
    "norms = np.sum(norms, axis=1)\n",
    "noisy_priorities = list(range(len(norms)))\n",
    "noisy_priorities.sort(key=lambda row: norms[row], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51467144 0.72026665 0.98916511 0.99979489 0.99999986]\n"
     ]
    }
   ],
   "source": [
    "base_angles = np.zeros((TOP_BASES, TOP_BASES))\n",
    "\n",
    "for row in range(TOP_BASES):\n",
    "    for col in range(TOP_BASES):\n",
    "        base_angles[row][col] = np.abs(noisy_dictionary[:,noisy_priorities[row]] @ \\\n",
    "                                       dictionary[:,clean_priorities[col]])\n",
    "        \n",
    "top_angles = []\n",
    "for index in range(TOP_BASES):\n",
    "    top_angles.append(max(base_angles[index]))\n",
    "\n",
    "values = np.percentile(top_angles, [0, 25, 50, 75, 100])\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
